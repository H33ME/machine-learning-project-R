---
title: 'Final Project: Predictive Analysis for Wage Data set from ISLR2 package in
  R'
subtitle: 'MATH 40028/50028: Statistical Learning'
date: "March 22, 2022"
output:
  pdf_document: default
  html_document:
    df_print: paged
fontfamily: mathpazo
fontsize: 11pt
header-includes: \linespread{1.05}
urlcolor: blue
---

__ACADEMIC INTEGRITY: Every student should complete the project by their own. A project report having high degree of similarity with work by any other student, or with any other document (e.g., found online) is considered plagiarism, and will not be accepted. The minimal consequence is that the student will receive the project score of 0, and the best possible overall course grade will be D. Additional consequences are described at http://www.kent.edu/policyreg/administrative-policy-regarding-student-cheating-and-plagiarism and will be strictly enforced.__

## Instruction

__Goal:__ The goal of the final project is to apply the statistical learning methods discussed in this course to perform predictive analysis of real-life data. You will need to identify prediction problem(s), carry out necessary exploratory data analysis, perform predictive analysis using statistical learning methods, assess the performance, and communicate the results in a report. 

__Report:__ Use this Rmd file as a template. Edit the file by adding your project title in the YAML, and including necessary information in the four sections: (1) Introduction, (2) Statistical learning strategies and methods, (3) Predictive analysis and results, and (4) Conclusion. 

__Submission:__ Please submit your project report as a PDF file (4-8 pages, flexible) to Canvas by __11:59 p.m. on May 4__. The PDF file should be generated by “knitting” the Rmd file. You may choose to first generate an HTML file (by changing the output format in the YAML to `output: html_document`) and then convert it to PDF. __20 points will be deducted if the submitted files are in wrong format.__ 

__Grade:__ The project will be graded based on your ability to (1) recognize and define prediction problems, (2) identify potentially useful statistical learning methods, (3) perform the predictive analysis and assess the performance, (4) document the analysis procedure (with R code) and clearly present the results, and (5) draw valid conclusions supported by the analysis.

__Datasets:__ You may consider (but are not restricted) to use the following packages/datasets. 

* [`ISLR2`](https://cran.rstudio.com/web/packages/ISLR2/ISLR2.pdf): datasets used in the _Introduction to Statistical Learning_ textbook
* [`dslabs`](https://cran.r-project.org/web/packages/dslabs/dslabs.pdf)

\pagebreak

## Introduction [15 points]

### Loading data and packages required

The first step is to load the necessary packages and data set. The "ISLR2" package, which contains the data, must be loaded in order to conduct this analysis. 'Wage' data will be used in the analysis. This data contains a sample of 3000 male workers in the Mid Atlantic region, including information on salaries and other relevant statistics.

```{r Loading packages and data, message=FALSE, warning=FALSE}
# set working directory
setwd('~/Final-Project')

# loading required packages
library(caret)
library(e1071)
library(ISLR2)
library(tidyverse)
library(randomForest)

# loading data to working space
wage_data <- ISLR2::Wage
```

### Target Population and sampling techniques used

The workforce in the Mid-Atlantic region, as shown by the wage data, would be the target group in this scenario. Predictive analysis uses a variety of sampling techniques to choose this data from a broader population. One of the sampling technique used is:

  In a simple random sampling strategy, each observation in the population has an equal chance of being chosen. This strategy is typically used when the population size is known and there are small datasets.

  
### Possible bias in predictive analysis.

Potential bias in predictive analysis include the following:

  1. Sampling bias occurs when conclusions about the population are drawn after an analysis of a sample that is not representative of the population from which it was drawn.

  2. Improper measurements or information collection methods that result in results that are erroneous or imprecise are known as measurement bias.

### Predictive Problem   

The goal is to predict a worker's comparable income or compensation based on a variety of demographic and employment related factors, including age, education, experience, and occupation.

### Splitting of the data

The data will then be divided into training and testing sets. We divided the data by 80/20, meaning that 80 percent of the data will be used for training and the remaining 20 percent for testing. This is seen below.

```{r split data into train and test}
set.seed(12345)
# index that split the data
train_index <-
  createDataPartition(wage_data$wage, p = 0.8, list = FALSE)
# train and test data
training_data <- wage_data[train_index, ]
testing_data <- wage_data[-train_index, ]
```

## Statistical learning strategies and methods [35 points]

### exploratory data analysis on the training data set

We do exploratory data analysis, or EDA, on the data used for training in this analysis in order to comprehend the distribution of the variables, identify any outliers or missing values, and determine the relationships between the predictors and response variables. The `boxplot.stats()` and `is.na()` routines are used to check for outliers and missing data, respectively, as illustrated below.

```{r EDA on training data}
# check the structure of data
str(training_data)
# check for missing data
table(is.na(training_data))
# check for outliers on the target variable wage
# use a boxplot
training_data %>%
  ggplot(aes(wage)) +
  geom_boxplot()
# our plot shows availability of outliers
# remove them using boxplot.stats()
outliers <- boxplot.stats(training_data$wage)$out

# filter out the outliers
training_data <- training_data %>%
  filter(!wage %in% outliers)

# check if the target variable is normally distributed
# using a histogram
training_data %>%
  ggplot(aes(wage)) +
  geom_histogram()
# the target variable is normally distributed
# explore interaction between dependent and independent variables
training_data %>%
  ggplot(aes(x = wage, age, color = education)) +
  geom_point() +
  facet_wrap( ~ race) +
  labs(title = 'A scatterplot showing distribution of wage on age, education and race', x = 'Target variable-Wage', y =
         'Age')
  
```

Using the `str()` function, we analyzed the data's structure, verified that there were no missing values, and then we chose the outliers in our target variable and removed these values from the data. According to the produced histogram, our target variable is regularly distributed. Finally, we used a scatter plot to examine if any factors were dependent on wage. We found that these variables did indeed depend on the target variable. As a result, we can draw the conclusion that the independent variables in our data rely on the desired variable.
    
### Statistical learning approach alongside other methods for feature engineering techniques

For feature engineering, a variety of statistical learning methods and techniques can be applied:

  1. By identifying the most significant features that account for the majority of the variance in the data, principal component analysis (PCA), a statistical technique, can minimize the dimensionality of the data. A set of variables that are correlated can be converted into a set of unrelated variables using PCA, and these uncorrelated variables can subsequently be used as features in machine learning techniques.

  2. Feature scaling and feature selection is the procedure of altering the data's scale such that it is comparable. This is significant because the size of the input data has an impact on many machine learning methods. By using techniques like normalization, standardization, or log transformation, features can be scaled.

Therefore in our case we perform feature selection on our data so that we can use it to train a final machine learning model that will lead to best predictions.

```{r Feature engineering to training data}
set.seed(12345)
#train random forest model and calculate feature importance
rf <- randomForest(x= training_data[,1:9],y= training_data[,11])
var_imp <- varImp(rf, scale = FALSE)
#sort the score in decreasing order
var_imp_df <- data.frame(cbind(variable = rownames(var_imp), score = var_imp[,1]))
var_imp_df$score <- as.double(var_imp_df$score)
var_imp_df[order(var_imp_df$score,decreasing = TRUE),]
# create a plot for the data frame above
ggplot(var_imp_df, aes(x=reorder(variable, score), y=score)) + 
  geom_point() +
  geom_segment(aes(x=variable,xend=variable,y=0,yend=score)) +
  ylab("IncNodePurity") +
  xlab("Variable Name") +
  coord_flip()
```
`Education`, `age`, `health_ins`, `year`, `maritl`, `race`, `jobclass`, and `health` are the variables that can be viewed as independent factors for the target variable. The final machine learning model will be based on these variables.
  
### application of statistical learning techniques to forecasting under their presumptive assumptions.

When using statistical learning techniques to solve a prediction problem, it is important to take into account their various underlying assumptions. For continuous goal variables with a relationship that is linear between the given input variables and the target, linear regression, for instance, is appropriate. Target variables that are binary or categorical respond well to logistic regression. Random forests can accommodate inter variable interactions, whereas decision trees presume unbiased variables as inputs with a non-linear connection to the target. While neural networks are appropriate for complicated and non linear interactions between input parameters and target, support vector machine models also assume independently input indicators with a non linear connection to the target.

## Predictive analysis and results [35 points]

### Steps in the Statistical Learning Procedure for Predictive Analysis

For the procedure for predictive analysis for the wage data, we are going to use the Random Forest method to create a model that will help us to determine a worker's equivalent wage given a set of demographic and job-related characteristics using the `randomForest()` function.A number of parameters are passed into the function along with the training data as input. This is done as shown below.
    
```{r random forest for data}
# create a model using randomforest method
# formula used

formula <-
  as.formula("wage~year+age+maritl+race+education+jobclass+health+health_ins")
set.seed(12345)

# create the model
rf_model <-
  randomForest(
    formula,
    data = training_data,
    ntree = 500,
    mtry = 3
  )

# predict using the model
predict_rf_model <- predict(rf_model, newdata = testing_data)

```

In the above lines of codes, we started by identifying the model's variables and created an object called `formula`. This object contains the target variable that we want to predict using the other independent variables. The model created is a Random Forest Learning model. The model helps us determine workers wage given other characteristics. Using the models we predicted values using the `predict` function. 

### Analyzing the effectiveness of statistical learning methods with test data

To check the effectiveness of the model, we evaluate the performance of the random forest model using the Root Mean Square Error(RMSE), Mean Square Error(MSE), and Mean Absolute Error(MAE).
    
```{r}
# evaluate the peformance of the model
rmse <- RMSE(predict_rf_model, testing_data$wage)
rsq <- R2(predict_rf_model, testing_data$wage)
MAE <- mean(abs(testing_data$wage - predict_rf_model))
cat('Mean Square Error:',rsq, '\nRoot Mean Square Error:',rmse ,'\nAverage Absolute Difference:', MAE)

```

## Conclusion [15 points]

### Scope and generalizability of predictive analysis

We can draw the following conclusions regarding the effectiveness of the Random Forest model based on the evaluation metrics:

  1. `r rsq` is the Mean Square Error (MSE): This shows that the target variable's average squared difference between projected values and actual values is `r rsq`. This indicates that, generally speaking, the model's predictions are fairly accurate.

  2. The average difference between the target variable's predicted and actual values is `r rmse`, according to the Root Mean Square Error (RMSE), which is the square root of the MSE. Given the range of values in the target variable, despite this figure being a little large, it is still reasonable.
 
### Potential limitations and possibilities for improvement.

The aforementioned random forest model might have shortcomings and room for development. The quality of the data used to train the model could be one restriction because errors, missing values, or outliers could have an impact on the model's precision. The selection of characteristics used to train the model could also have limitations because doing so could make the model less effective by using redundant or unnecessary features. For instance, changing the quantity of trees, their depth, and the number of variables chosen for each split can help to maximize the performance of the model. Addressing these potential restrictions and assessing the model's performance on independent test datasets may be beneficial in enhancing the model's overall performance.